{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupo PySpark - Database\n",
    "\n",
    "Nuestro grupo se encargará de crear la base de datos en la nube y los cluster de PySpark para procesar los datos almacenados en esa base.\n",
    "\n",
    "Para la creación de la base de datos utilizaremos Databricks. Vamos paso a paso."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Nos creamos una cuenta en Databricks Community Edition (It's free!):\n",
    "\n",
    "<img src=\"./img/Crear_Cuenta_Databricks.png\" alt=\"Creación de cuenta en Databricks\" width=\"1000\"/>\n",
    "\n",
    "2. Clicamos en 'Get started with Community Edition':\n",
    "\n",
    "<img src=\"./img/Crear_Cuenta_Databricks_2.png\" alt=\"Creación de cuenta en Databricks 2\" width=\"1000\"/>\n",
    "\n",
    "3. Realizamos el captcha (ya sabeis, ayudamos a entrenar a las modelos jejeje) y nos mandarán un correo de conmfirmación para validar nuestra cuenta.\n",
    "\n",
    "4. Al dar en el link, nos encontraremos en el menú de databricks Community Edition:\n",
    "\n",
    "<img src=\"./img/Databricks_Menu.png\" alt=\"Menu de Databricks\" width=\"1500\"/>\n",
    "\n",
    "5. Lo primero será crear un cluster de PySpark. Para ello accederemos al menú vertical izquierdo y clicaremos en \"Create\" > \"Cluster\":\n",
    "\n",
    "<img src=\"./img/Crear_Cluster_1.png\" alt=\"Crear Cluster\" width=\"1500\"/>\n",
    "\n",
    "6.  Accederemos a este menú:\n",
    "\n",
    "<img src=\"./img/Crear_Cluster_2.png\" alt=\"Configurar Cluster\" width=\"1500\"/>\n",
    "\n",
    "7. Aquí deberemos de asignar el nombre que queremos darle al cluster, seleccionar la versión de Spark y el tipo de nodo que se ajuste a las necesidades de los datos, el tipo de instancia, la configuración de PySpark, las variables de entorno...\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y te preguntarás, ¿Que es un cluster de PySpark? ¿Porque lo necesitamos y para que sirve?\n",
    "\n",
    "Un cluster de PySpark es un conjunto de máquinas interconectadas que se utilizan para procesar grandes volúmenes de datos utilizando la biblioteca de procesamiento distribuido de PySpark. Cada máquina en el clúster se llama nodo y puede procesar datos y ejecutar tareas de forma independiente o en conjunto con otras máquinas en el clúster. Deberemos de ajustar el tipo de nodo y la cantidad de nodos que queremos utilizar en base a nuestros datos. A mayor volumen de datos, mas nodos y más almacenamiento. En operaciones que requieran mucho CPU, como minería de datos o procesamiento de imágenes, necesitaremos nodos de CPU más rápidos.\n",
    "\n",
    "PySpark utiliza el modelo de programación MapReduce para procesar grandes conjuntos de datos y proporciona una interfaz de programación de aplicaciones (API) en Python para interactuar con el clúster. El clúster de PySpark se utiliza comúnmente para procesar grandes conjuntos de datos en paralelo y en tiempo real, lo que lo hace adecuado para aplicaciones de big data y aprendizaje automático. Por así decirlo, es muy similar a Pandas pero para volumenes de datos mucho más grandes.\n",
    "\n",
    "Los clústeres de PySpark se pueden configurar en la nube o en un centro de datos local y pueden escalar horizontalmente o verticalmente según las necesidades del usuario. También se pueden configurar para ejecutarse en modo local en una sola máquina para pruebas y desarrollo de aplicaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Una vez creado el cluster, debemos de crear la base de datos clicando en Data en el panel de la izquierda y seleccionamos la fuente de datos que queremos volcar en esa base de datos y le pondremos un nombre.\n",
    "\n",
    "<img src=\"./img/Crear_Database.png\" alt=\"Crear Database\" width=\"1500\"/>\n",
    "\n",
    "Una vez montada la base de datos con nuestros datos, podremos crear una instancia de SparkSession con PySpark, crear un dataframe de PySpark y realizar consultas, transformaciones y, en definitva, procesar los datos con las funciones de PySpark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos con PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero de todo, ¿Que es PySpark?\n",
    "\n",
    "- PySpark es una API de Spark con Python que \"expone\" el modelo de programación de Spark a Python. ¿Que siginifica esto? Spark es una plataforma de procesamiento de datos que se constituye mediante dos lenguajes de programación, Java y Scala. PySpark es una biblioteca que proporciona una capa de abstracción en Python sobre estos lenguajes, lo que permite al desarrollador interactuar con Spark mediante el Python, sin necesidad de usar Scala o Java, mediante una API.\n",
    "\n",
    "### Vamos con una cheatsheet con lo básico de PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando PySpark\n",
    "\n",
    "1. Para inicializar Spark, debes proporcionar la URL del nodo maestro de Spark al constructor de SparkContext. Esta URL puede estar en uno de los siguientes formatos:\n",
    "\n",
    "    - \"local\": Ejecuta Spark localmente con un único hilo de trabajo.\n",
    "    - \"local[N]\": Ejecuta Spark localmente con N hilos de trabajo.\n",
    "    - \"spark://HOST:PORT\": Conéctate a un clúster independiente de Spark con el host y el número de puerto especificados.\n",
    "    - \"mesos://HOST:PORT\": Conéctate a un clúster de Mesos con el host y el número de puerto especificados.\n",
    "    - \"yarn\": Conéctate a un clúster YARN y utiliza la configuración de Hadoop para determinar el gestor de recursos y el número de núcleos del ejecutor.\n",
    "        - Recuerda que al hacer el cluster de PySpark al construir la base de datos, defines el número de nodos y el tipo de nodos en base a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(master=\"local\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Atributos y métodos de un objeto 'SparkContext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version # Devuelve la versión de Spark que se está ejecutando.\n",
    "sc.pythonVer # Devuelve la versión de Python que se está utilizando para ejecutar el código de Spark.\n",
    "sc.master # Devuelve la URL del nodo maestro de Spark al que se ha conectado el SparkContext.\n",
    "str(sc.sparkHome) # Devuelve la ruta de acceso al directorio principal de Spark en los nodos de trabajo en forma de string.\n",
    "str(sc.sparkUser()) # Devuelve el nombre de usuario de Spark que está ejecutando el SparkContext en forma de string.\n",
    "sc.appName # Devuelve el nombre de la aplicación Spark.\n",
    "sc.applicationId # Devuelve el identificador único de la aplicación Spark.\n",
    "sc.defaultParallelism # Devuelve el nivel de paralelismo predeterminado utilizado por Spark. Este valor se utiliza cuando no se especifica ningún nivel de paralelismo para las operaciones de RDD.\n",
    "sc.defaultMinPartitions # Devuelve el número mínimo de particiones que se crearán por defecto para los RDDs si no se especifica ningún número de particiones en las operaciones de creación de RDD.\n",
    "\n",
    "# RDD se refiere a \"Resilient Distributed Dataset\" (Conjunto de datos distribuido y resistente) y es el nombre que se le da al conjunto de datos, en nuestro caso, extraido de la base de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Podemos configurar un contexto de Spark en PySpark utilizando un objeto SparkConf. La configuración se establece utilizando los métodos set en el objeto SparkConf, y luego se crea una instancia de SparkContext utilizando esta configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .setMaster('local') # Especifica que se debe crear un contexto de Spark en modo local (es decir, no en un clúster).\n",
    "        .setAppName('My app') # Establece el nombre de la aplicación a \"My app\"\n",
    "        .set('spark.executor.memory','1g') # Establece la cantidad de memoria asignada a cada executor en 1 GB.\n",
    "        )\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos\n",
    "\n",
    "1. Podemos crear instancias de RDDs (Resilient Distributed Datasets) a partir de colecciones paralelizadas en Spark:\n",
    "\n",
    "    - ¿Que es una colección paralelizada? Es una forma de representar un conjunto de datos en Spark de modo que pueda ser procesado en paralelo en un cluster de nodos. En lugar de crear una colección de datos en la memoria de un solo nodo, se crean múltiples copias de la colección en diferentes nodos del cluster, permitiendo que los datos sean procesados de forma distribuida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a', 7), ('a', 2), ('b', 2)])\n",
    "\n",
    "rdd2 = sc.parallelize([('a', 2), ('d', 1), ('b', 1)])\n",
    "\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "\n",
    "rdd4 = sc.parallelize([('a', ['x', 'y', 'z']), ('b', ['p', 'r'])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tambien podemos leer datos externos en Spark, como archivos de texto, y procesarlos en paralelo:\n",
    "\n",
    "    - La función textFile() de PySpark se utiliza para leer uno o más archivos de texto plano desde un sistema de archivos compatible con Hadoop, como HDFS o el sistema de archivos local. Puede tomar una ruta de archivo o un patrón de búsqueda que coincida con varios archivos. La función devuelve un RDD donde cada registro del RDD corresponde a una línea del archivo de texto.\n",
    "    - La función wholeTextFiles() también se utiliza para leer archivos de texto, pero en lugar de devolver un RDD con líneas de texto, devuelve un RDD de pares clave-valor donde la clave es la ruta del archivo y el valor es el contenido completo del archivo como una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile( \"/my/directory/*.txt\") # Crea un RDD que contendrá todas las líneas de texto de todos los archivos con extensión \".txt\" en el directorio \"/my/directory/\".\n",
    "\n",
    "textFile2 = sc.wholeTextFiles(\"/my/directory/\") # Crea un RDD de pares clave-valor donde la clave es la ruta del archivo y el valor es el contenido completo del archivo en formato de cadena."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación de la información del RDD\n",
    "\n",
    "1. Información básica:\n",
    "\n",
    "    - Funciones aplicables a los RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.getNumPartitions() # Devuelve una lista del número de particiones en el RDD.\n",
    "rdd.count() # Cuenta el número de elementos en el RDD.\n",
    "rdd.countByKey() # Cuenta el número de elementos por clave en el RDD y devuelve los resultados como un diccionario.\n",
    "rdd.countByValue() # cuenta el número de elementos por valor en el RDD y devuelve los resultados como un diccionario.\n",
    "rdd.collectAsMap() # Devuelve los elementos del RDD como un diccionario.\n",
    "rdd3.sum() # Devuelve la suma de los elementos en el RDD. Tienen que ser RDD numéricos, como por ejemplo: rdd3 = sc.parallelize(range(100)).\n",
    "sc.parallelize([]).isEmpty() # Devuelve True si el RDD está vacío, False en caso contrario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Valores estadísticos de los RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.max() # Devuelve el valor máximo del RDD.\n",
    "rdd3.min() # Devuelve el valor mínimo del RDD.\n",
    "rdd3.mean() # Devuelve el valor medio (promedio) del RDD.\n",
    "rdd3.stdev() # Devuelve la desviación estándar del RDD.\n",
    "rdd3.variance() # Devuelve la varianza del RDD.\n",
    "rdd3.histogram(3) # Devuelve un par de listas, la primera con los límites de los tres intervalos de binning y la segunda con el número de elementos en cada intervalo.\n",
    "rdd3.stats() # Devuelve un objeto 'StatCounter' que contiene las estadísticas resumidas del RDD: número de elementos, la media, la varianza, la desviación estándar, el valor máximo y el valor mínimo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando funciones (a RDDs)\n",
    "\n",
    "1.  rdd.map('funcion lambda').collect()\n",
    "\n",
    "        - Aplica una función lambda a cada elemento del RDD y devuelve un nuevo RDD con los resultados. En este caso, la función toma cada tupla y crea una nueva tupla que contiene los mismos elementos más los dos elementos de la tupla intercambiados entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.map(lambda x: x+(x[1],x[0])).collect() # [('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. rdd.flatMap('funcion lambda').collect(): \n",
    "\n",
    "        - Aplica una función lambda a cada elemento del RDD y devuelve un nuevo RDD con los resultados, pero aplana la salida para que no haya anidamiento de elementos. En este caso, la función toma cada tupla y crea una lista con los mismos elementos más los dos elementos de la tupla intercambiados entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.flatMap(lambda x: x+(x[1],x[0])).collect() # ['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. rdd.flatMapValues('funcion lambda'). collect():\n",
    "\n",
    "        - Aplica una función lambda a cada valor de cada par clave-valor del RDD y devuelve un nuevo RDD con los resultados, manteniendo las claves. En este caso, la función toma cada valor (que es una lista) y devuelve cada elemento de la lista con la misma clave que tenía el valor original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4.flatMapValues(lambda x: x).collect() # [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de datos (de RDDs)\n",
    "\n",
    "1. Obtención de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect() # Devuelve una lista con todos los elementos del RDD.\n",
    "rdd.take(int) # Devuelve una lista con los primeros n elementos del RDD.\n",
    "rdd.first() # Devuelve el primer elemento del RDD.\n",
    "rdd.top(int) # Devuelve una lista con los n elementos más grandes del RDD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtención de muestras aleatorias:\n",
    "\n",
    "- El método sample() se utiliza para tomar una muestra aleatoria de un RDD. Toma tres argumentos:\n",
    "\n",
    "    - withReplacement (booleano): indica si los elementos muestreados deben ser reemplazados en el RDD original.\n",
    "    - fraction (float): la fracción de elementos que se deben muestrear, en el rango de 0 a 1.\n",
    "    - seed (int): semilla aleatoria para la generación de números aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.sample(False, 0.15, 81).collect() \n",
    "\n",
    "# En este ejemplo, toma una muestra aleatoria del RDD rdd3 con:\n",
    "# withReplacement = False (Sin reemplazo), \n",
    "# fraction = 0.15 (La muestra será del 15% de sus elementos), \n",
    "# seed = 81 (semilla aleatoria 81).\n",
    "# El método collect() devuelve la muestra como una lista de elementos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Filtrado de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.filter(lambda x: 'a' in x).collect() # Filtra los elementos del RDD para obtener solo aquellos que contienen la letra 'a', en este caso.\n",
    "rdd.distinct().collect() # Retorna un nuevo RDD con sus elementos únicos.\n",
    "rdd.keys().collect() # Retorna una lista con todas las claves del RDD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteraciones\n",
    "\n",
    "La función foreach() aplica una función dada a cada elemento del RDD, pero a diferencia de otras operaciones, no devuelve nada. En su lugar, se utiliza para ejecutar un código personalizado para cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x): print(x) # Definimos la función g, que se utilizará de argumento para la función de iteración de RDD.\n",
    "\n",
    "rdd.foreach(g) # Esta función itera la función dada sobre cada uno de los elementos del RDD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remodelación de datos (en RDDs)\n",
    "\n",
    "1. Reducción de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduceByKey('función lambda').collect() # Reduce la RDD utilizando una función de reducción y devuelve una lista de tuplas de clave-valor reducidas.\n",
    "\n",
    "rdd.reduce('función lambda').collect() # Reduce la RDD utilizando una función de reducción y devuelve un solo valor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Agrupación de datos mediante Group By:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.groupBy('función lambda').mapValues(list).collect() # Agrupa elementos del RDD en base a la función introducida como argumento, generando pares (clave, iterador).\n",
    "\n",
    "rdd.groupByKey().mapValues(list).collect() # Agrupa elementos del RDD por su clave, generando pares (clave, iterador)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Agregación de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1)) # Función que se utiliza para combinar dos valores de un RDD (Operación secuencial).\n",
    "\n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1])) # Función que se utiliza para combinar los resultados de varias particiones (Operación de combinación).\n",
    "\n",
    "rdd3.aggregate((0,0),seqOp,combOp) # La función aggregate toma tres argumentos: el valor inicial (una tupla con dos elementos en este caso), la función seqOp y la función combOp.\n",
    "\n",
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect() #  La función aggregateByKey es similar a aggregate, pero se aplica a un RDD clave-valor.\n",
    "\n",
    "rdd3.fold(0,'add') # La función fold toma dos argumentos: el valor inicial y una función binaria que combina dos valores, en este caso 'add', que los suma.\n",
    "\n",
    "rdd.foldByKey(0, 'add').collect() # La función foldByKey es similar a fold, pero se aplica a un RDD clave-valor.\n",
    "\n",
    "rdd3.keyBy(lambda x: x+x).collect() # La función keyBy crea un nuevo RDD donde cada elemento es una tupla con el valor original y su doble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones matemáticas (sobre RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.subtract(rdd2).collect() # Esta operación devuelve un nuevo RDD con los elementos que aparecen en el RDD de origen pero no en el RDD de destino.\n",
    "\n",
    "rdd2.subtractByKey(rdd).collect() # Esta operación devuelve un nuevo RDD que contiene solo las claves presentes en el RDD de origen que no aparecen en el RDD de destino.\n",
    "\n",
    "rdd.cartesian(rdd2).collect() # Esta operación devuelve el producto cartesiano de dos RDDs. Es decir, devuelve todos los pares posibles entre los elementos de los RDDs de origen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordenar datos (sobre RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.sortBy('función lambda').collect() # La función sortBy ordena los elementos de un RDD en función de una función lambda dada en orden ascendente o descendente.\n",
    "\n",
    "rdd2.sortByKey().collect() # La función sortByKey ordena los elementos de un RDD en función de una clave en orden ascendente o descendente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartición de datos (sobre RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.repartition(int) # Redistribuye los datos en un RDD en n particiones, mejora la paralelización y el rendimiento de las operaciones subsiguientes que se realicen en el RDD. \n",
    "# Esta operación NO reduce el número de particiones.\n",
    "\n",
    "rdd.coalesce(1) # Reduce el número de particiones en un RDD a una sola. \n",
    "# A diferencia de repartition(), esta operación no realiza una redistribución de datos, por tanto, NO implica una transferencia de datos a través de la red,\n",
    "# lo que la hace MÁS eficiente en términos de tiempo y recursos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar datos (sobre RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"rdd.txt\") # Esta función guarda el RDD como un archivo de texto en la ruta especificada. \n",
    "# Spark creará un archivo en la ruta de salida especificada y escribirá el contenido del RDD como texto en el archivo.\n",
    "\n",
    "rdd.saveAsNewAPIHadoopFile(\"hdfs://namenodehost/parent/child\",'org.apache.hadoop.mapred.TextOutputFormat') # Esta función utiliza la API YARN de Hadoop para escribir datos en HDFS y guardarlos en la ruta especificada. \n",
    "# El formato de salida debe especificarse mediante una clase de formato de salida de Hadoop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cerrar el SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mucho texto \n",
    "\n",
    "En resumen, ¿como procedemos a trabajar con la base de datos?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importación de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Crear una instancia de SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"nombre_de_la_aplicacion\") \\\n",
    "        .config(\"spark.some.config.option\", \"valor_de_la_opcion\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cargar datos en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "     .option(\"header\", \"true\") \\\n",
    "     .option(\"inferSchema\", \"true\") \\\n",
    "     .load(\"ruta_del_archivo.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Mostrar los primeros n registros de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Mostrar el esquema de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Seleccionar columnas de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"columna_1\", \"columna_2\", ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Filtrar datos de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"condicion\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Agrupar y contar datos en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"columna\").count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Ordenar datos de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(\"columna\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Unir dos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, \"clave_comun\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Agregar una nueva columna a un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"nombre_nueva_columna\", \"expresion\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Convertir un Spark DataFrame a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Escribir un DataFrame a un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").save(\"ruta_del_archivo.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Detener una instancia de SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
